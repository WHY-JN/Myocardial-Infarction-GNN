{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T01:50:40.470231Z","iopub.execute_input":"2025-08-27T01:50:40.470604Z","iopub.status.idle":"2025-08-27T01:50:43.025557Z","shell.execute_reply.started":"2025-08-27T01:50:40.470571Z","shell.execute_reply":"2025-08-27T01:50:43.024654Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install ucimlrepo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T01:52:40.368283Z","iopub.execute_input":"2025-08-27T01:52:40.368615Z","iopub.status.idle":"2025-08-27T01:52:46.035547Z","shell.execute_reply.started":"2025-08-27T01:52:40.368591Z","shell.execute_reply":"2025-08-27T01:52:46.034558Z"}},"outputs":[{"name":"stdout","text":"Collecting ucimlrepo\n  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.3)\nRequirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.6.15)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas>=1.0.0->ucimlrepo) (2024.2.0)\nDownloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\nInstalling collected packages: ucimlrepo\nSuccessfully installed ucimlrepo-0.0.7\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\n# 引入 KFold, StratifiedKFold 是更适合不均衡数据集的版本\nfrom sklearn.model_selection import StratifiedKFold\nfrom ucimlrepo import fetch_ucirepo\nfrom sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score, recall_score, confusion_matrix, balanced_accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport math\n\n# --- 引入 Scikit-learn 模型 ---\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\n\n# --- 定义设备 ---\ndevice = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\nprint(f\"Using device: {device}\")\n\n# --- Focal Loss (用于 PyTorch MLP 模型) ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.pos_weight = pos_weight\n\n    def forward(self, inputs, targets):\n        current_pos_weight = self.pos_weight\n        if current_pos_weight is not None and current_pos_weight.device != inputs.device:\n             current_pos_weight = current_pos_weight.to(inputs.device)\n\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none', pos_weight=current_pos_weight)\n        probs = torch.sigmoid(inputs)\n        pt = torch.where(targets == 1, probs, 1 - probs)\n        \n        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha).to(inputs.device)\n        focal_weight = alpha_t * torch.pow(1 - pt, self.gamma)\n        loss = focal_weight * BCE_loss\n        \n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n\n# --- MLP 模型 (PyTorch) ---\nclass MLPNet(nn.Module):\n    def __init__(self, num_features, hidden_channels, num_classes, dropout_rate=0.4):\n        super(MLPNet, self).__init__()\n        self.fc1 = nn.Linear(num_features, hidden_channels)\n        self.bn1 = nn.BatchNorm1d(hidden_channels)\n        self.fc2 = nn.Linear(hidden_channels, hidden_channels // 2)\n        self.bn2 = nn.BatchNorm1d(hidden_channels // 2)\n        self.fc3 = nn.Linear(hidden_channels // 2, num_classes)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout(x)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n# --- 数据准备 (与之前相同) ---\nmyocardial_infarction_complications = fetch_ucirepo(id=579)\nX_df_orig = myocardial_infarction_complications.data.features.copy()\ny_df_orig = myocardial_infarction_complications.data.targets.copy()\ntarget_names = list(y_df_orig.columns)\n\n# ... (数据清洗和特征选择部分与原代码完全相同，此处省略以保持简洁) ...\nfeature_collection_time = {\n    'AGE': 'admission', 'SEX': 'admission', 'INF_ANAM': 'admission', 'STENOK_AN': 'admission',\n    'FK_STENOK': 'admission', 'IBS_POST': 'admission', 'IBS_NASL': 'admission', 'GB': 'admission',\n    'SIM_GIPERT': 'admission', 'DLIT_AG': 'admission', 'ZSN_A': 'admission', 'nr_11': 'admission',\n    'nr_01': 'admission', 'nr_02': 'admission', 'nr_03': 'admission', 'nr_04': 'admission',\n    'nr_07': 'admission', 'nr_08': 'admission', 'np_01': 'admission', 'np_04': 'admission',\n    'np_05': 'admission', 'np_07': 'admission', 'np_08': 'admission', 'np_09': 'admission',\n    'np_10': 'admission', 'endocr_01': 'admission', 'endocr_02': 'admission', 'endocr_03': 'admission',\n    'zab_leg_01': 'admission', 'zab_leg_02': 'admission', 'zab_leg_03': 'admission',\n    'zab_leg_04': 'admission', 'zab_leg_06': 'admission', 'S_AD_KBRIG': 'admission',\n    'D_AD_KBRIG': 'admission', 'S_AD_ORIT': 'admission', 'D_AD_ORIT': 'admission',\n    'O_L_POST': 'admission', 'K_SH_POST': 'admission', 'MP_TP_POST': 'admission',\n    'SVT_POST': 'admission', 'GT_POST': 'admission', 'FIB_G_POST': 'admission',\n    'ant_im': 'admission', 'lat_im': 'admission', 'inf_im': 'admission', 'post_im': 'admission',\n    'IM_PG_P': 'admission', 'ritm_ecg_p_01': 'admission', 'ritm_ecg_p_02': 'admission',\n    'ritm_ecg_p_04': 'admission', 'ritm_ecg_p_06': 'admission', 'ritm_ecg_p_07': 'admission',\n    'ritm_ecg_p_08': 'admission', 'n_r_ecg_p_01': 'admission', 'n_r_ecg_p_02': 'admission',\n    'n_r_ecg_p_03': 'admission', 'n_r_ecg_p_04': 'admission', 'n_r_ecg_p_05': 'admission',\n    'n_r_ecg_p_06': 'admission', 'n_r_ecg_p_08': 'admission', 'n_r_ecg_p_09': 'admission',\n    'n_r_ecg_p_10': 'admission', 'n_p_ecg_p_01': 'admission', 'n_p_ecg_p_03': 'admission',\n    'n_p_ecg_p_04': 'admission', 'n_p_ecg_p_05': 'admission', 'n_p_ecg_p_06': 'admission',\n    'n_p_ecg_p_07': 'admission', 'n_p_ecg_p_08': 'admission', 'n_p_ecg_p_09': 'admission',\n    'n_p_ecg_p_10': 'admission', 'n_p_ecg_p_11': 'admission', 'n_p_ecg_p_12': 'admission',\n    'fibr_ter_01': 'admission', 'fibr_ter_02': 'admission', 'fibr_ter_03': 'admission',\n    'fibr_ter_05': 'admission', 'fibr_ter_06': 'admission', 'fibr_ter_07': 'admission',\n    'fibr_ter_08': 'admission', 'GIPO_K': 'admission', 'K_BLOOD': 'admission',\n    'GIPER_NA': 'admission', 'NA_BLOOD': 'admission', 'ALT_BLOOD': 'admission',\n    'AST_BLOOD': 'admission', 'KFK_BLOOD': 'admission', 'L_BLOOD': 'admission',\n    'ROE': 'admission', 'TIME_B_S': 'admission', 'NA_KB': 'admission', 'NOT_NA_KB': 'admission',\n    'LID_KB': 'admission', 'NITR_S': 'admission', 'LID_S_n': 'admission',\n    'B_BLOK_S_n': 'admission', 'ANT_CA_S_n': 'admission', 'GEPAR_S_n': 'admission',\n    'ASP_S_n': 'admission', 'TIKL_S_n': 'admission', 'TRENT_S_n': 'admission',\n}\nadmission_features = [f for f, t in feature_collection_time.items() if t == 'admission' and f in X_df_orig.columns]\nX_processed = X_df_orig[admission_features].copy()\nfor col in admission_features:\n    if X_processed[col].isnull().any():\n        X_processed[col].fillna(X_processed[col].median(), inplace=True)\n\ny_processed = y_df_orig.copy()\nfor col in y_processed.columns:\n    if y_processed[col].isnull().any():\n        y_processed[col].fillna(y_processed[col].median(), inplace=True)\n    y_processed[col] = y_processed[col].astype(int)\n\nlabel_11_col_name = y_processed.columns[10]\ny_processed[label_11_col_name] = (y_processed[label_11_col_name] > 0).astype(int)\n\n# --- 数据标准化和准备 ---\n# 特征缩放器现在将在交叉验证循环之外定义，以对整个数据集进行拟合\n# 这样可以确保每一折都使用相同的缩放标准\nscaler = StandardScaler()\nX_scaled_np = scaler.fit_transform(X_processed)\ny_np = y_processed.values\n\n# --- 评估和绘图函数 (与之前相同) ---\ndef calculate_metrics(all_labels_np, all_preds_prob_np):\n    # ... (此函数与之前完全相同) ...\n    threshold = 0.5\n    all_preds_binary_np = (all_preds_prob_np > threshold).astype(int)\n\n    num_labels_total = all_labels_np.shape[1]\n    metrics = {\n        'per_label_acc': [], 'per_label_recall': [], 'per_label_specificity': [],\n        'per_label_ap': [], 'per_label_roc_auc': [], 'per_label_balanced_acc': [],\n        'confusion_matrices': []\n    }\n\n    for i in range(num_labels_total):\n        true_1d = all_labels_np[:, i]\n        prob_1d = all_preds_prob_np[:, i]\n        pred_1d = all_preds_binary_np[:, i]\n        unique_true = np.unique(true_1d)\n\n        acc = accuracy_score(true_1d, pred_1d)\n        metrics['per_label_acc'].append(acc)\n\n        if len(unique_true) < 2:\n            metrics['per_label_recall'].append(np.nan)\n            metrics['per_label_specificity'].append(np.nan)\n            metrics['per_label_ap'].append(np.nan)\n            metrics['per_label_roc_auc'].append(np.nan)\n            metrics['per_label_balanced_acc'].append(np.nan if np.isnan(acc) else acc)\n            if unique_true[0] == 0: tn, fp, fn, tp = np.sum(pred_1d == 0), np.sum(pred_1d == 1), 0, 0\n            else: tn, fp, fn, tp = 0, 0, np.sum(pred_1d == 0), np.sum(pred_1d == 1)\n            metrics['confusion_matrices'].append((tn, fp, fn, tp))\n            continue\n\n        cm = confusion_matrix(true_1d, pred_1d, labels=[0, 1])\n        tn, fp, fn, tp = cm.ravel()\n        metrics['confusion_matrices'].append((tn, fp, fn, tp))\n        \n        recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n        specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n        balanced_acc = (recall + specificity) / 2 if not (np.isnan(recall) or np.isnan(specificity)) else np.nan\n        \n        metrics['per_label_recall'].append(recall)\n        metrics['per_label_specificity'].append(specificity)\n        metrics['per_label_balanced_acc'].append(balanced_acc)\n\n        try:\n            ap = average_precision_score(true_1d, prob_1d)\n            roc_auc = roc_auc_score(true_1d, prob_1d)\n        except ValueError:\n            ap, roc_auc = np.nan, np.nan\n        metrics['per_label_ap'].append(ap)\n        metrics['per_label_roc_auc'].append(roc_auc)\n        \n    final_metrics = {\n        'mean_acc': np.nanmean(metrics['per_label_acc']),\n        'mean_recall': np.nanmean(metrics['per_label_recall']),\n        'mean_specificity': np.nanmean(metrics['per_label_specificity']),\n        'mean_ap': np.nanmean(metrics['per_label_ap']),\n        'mean_roc_auc': np.nanmean(metrics['per_label_roc_auc']),\n        'mean_balanced_acc': np.nanmean(metrics['per_label_balanced_acc']),\n    }\n    return final_metrics\n\ndef evaluate_sklearn_model(model, X_data_np, y_data_np):\n    preds_prob_list = model.predict_proba(X_data_np)\n    preds_prob_np = np.hstack([p[:, 1].reshape(-1, 1) for p in preds_prob_list])\n    return calculate_metrics(y_data_np, preds_prob_np)\n\ndef evaluate_pytorch_model(model, X_data, y_data):\n    model.eval()\n    X_data, y_data = X_data.to(device), y_data.to(device)\n    with torch.no_grad():\n        out = model(X_data)\n        preds_prob = torch.sigmoid(out)\n    return calculate_metrics(y_data.cpu().numpy(), preds_prob.cpu().numpy())\n\ndef plot_performance_bar_chartjs(aggregated_results):\n    os.makedirs('results', exist_ok=True)\n    models = list(aggregated_results.keys())\n    \n    # 提取平均值用于绘图\n    datasets_dict = {\n        \"Mean ROC-AUC\": {\"data\": [aggregated_results[b]['mean_roc_auc']['mean'] for b in models], \"backgroundColor\": \"rgba(255, 99, 132, 0.7)\"},\n        \"Mean Balanced Acc\": {\"data\": [aggregated_results[b]['mean_balanced_acc']['mean'] for b in models], \"backgroundColor\": \"rgba(255, 159, 64, 0.7)\"},\n        \"Mean Recall\": {\"data\": [aggregated_results[b]['mean_recall']['mean'] for b in models], \"backgroundColor\": \"rgba(54, 162, 235, 0.7)\"},\n        \"Mean AP\": {\"data\": [aggregated_results[b]['mean_ap']['mean'] for b in models], \"backgroundColor\": \"rgba(255, 206, 86, 0.7)\"}\n    }\n    chart_config = {\"type\": \"bar\", \"data\": {\"labels\": models, \"datasets\": [{\"label\": k, **v} for k,v in datasets_dict.items()]},\n                    \"options\": {\"responsive\": True, \"maintainAspectRatio\": False, \n                                \"plugins\": {\"title\": {\"display\": True, \"text\": \"Model Performance Comparison (5-Fold CV)\"},\n                                            \"legend\": {\"position\": \"top\"}},\n                                \"scales\": {\"y\": {\"beginAtZero\": True, \"title\": {\"display\": True, \"text\": \"Score\"}}}}}\n    with open('results/overall_performance_summary_chartjs.json', 'w') as f:\n        json.dump(chart_config, f, indent=4)\n\n\n# --- 交叉验证主循环 ---\nN_SPLITS = 5\nRANDOM_STATE = 42\n\n# 用于分层的标签：1表示至少有一个并发症，0表示没有\nstratify_labels = (y_np.sum(axis=1) > 0).astype(int)\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n\nmodel_configs = {\n    'LogisticRegression': {\n        'model': LogisticRegression, 'params': {'solver': 'liblinear', 'random_state': RANDOM_STATE, 'max_iter': 200}, 'type': 'sklearn'\n    },\n    'XGBoost': {\n        'model': XGBClassifier, 'params': {'use_label_encoder': False, 'eval_metric': 'logloss', 'random_state': RANDOM_STATE, 'n_estimators': 150}, 'type': 'sklearn'\n    },\n    'CatBoost': {\n        'model': CatBoostClassifier, 'params': {'random_state': RANDOM_STATE, 'verbose': 0, 'iterations': 200}, 'type': 'sklearn'\n    },\n    'MLP': {\n        'model': MLPNet, 'params': {'hidden_channels': 128, 'dropout_rate': 0.4}, 'type': 'pytorch'\n    }\n}\n\n# 存储每个模型在每一折上的结果\nall_model_fold_results = {model_name: [] for model_name in model_configs.keys()}\n\n# --- 开始5折交叉验证 ---\nfor fold, (train_indices, test_indices) in enumerate(skf.split(X_scaled_np, stratify_labels)):\n    print(f\"\\n===== 开始处理第 {fold+1}/{N_SPLITS} 折 =====\")\n\n    # 1. 根据当前折的索引准备数据\n    X_train, y_train = X_scaled_np[train_indices], y_np[train_indices]\n    X_test, y_test = X_scaled_np[test_indices], y_np[test_indices]\n    \n    # 2. 为PyTorch模型准备Tensors\n    X_train_tensor, y_train_tensor = torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float)\n    X_test_tensor, y_test_tensor = torch.tensor(X_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.float)\n\n    # 3. 遍历并训练/评估每个模型\n    for model_name, config in model_configs.items():\n        print(f\"--- 正在处理模型: {model_name} (第 {fold+1} 折) ---\")\n\n        if config['type'] == 'sklearn':\n            # --- Scikit-learn 模型训练和评估 ---\n            base_model = config['model'](**config['params'])\n            model = MultiOutputClassifier(base_model)\n            model.fit(X_train, y_train)\n            test_result_dict = evaluate_sklearn_model(model, X_test, y_test)\n        \n        elif config['type'] == 'pytorch':\n            # --- PyTorch MLP 训练和评估 ---\n            # !! 关键：每一折都必须重新初始化模型、优化器等\n            num_features = X_train.shape[1]\n            num_classes = y_train.shape[1]\n            model = config['model'](num_features=num_features, num_classes=num_classes, **config['params']).to(device)\n            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n            \n            # 动态计算当前训练集的 pos_weight\n            num_positives_train = y_train_tensor.sum(axis=0)\n            num_negatives_train = len(y_train_tensor) - num_positives_train\n            pos_weight = num_negatives_train / (num_positives_train + 1e-8)\n            \n            criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction='mean', pos_weight=pos_weight)\n\n            epochs = 150 # 减少epoch数量，因为我们现在有5次训练\n            \n            for epoch in range(1, epochs + 1):\n                model.train()\n                optimizer.zero_grad()\n                out = model(X_train_tensor.to(device))\n                loss = criterion(out, y_train_tensor.to(device))\n                loss.backward()\n                optimizer.step()\n                if epoch % 50 == 0:\n                     print(f'    Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}')\n            \n            # 在当前折的测试集上评估\n            test_result_dict = evaluate_pytorch_model(model, X_test_tensor, y_test_tensor)\n\n        print(f\"    {model_name} (第 {fold+1} 折) 测试结果: ROC-AUC={test_result_dict['mean_roc_auc']:.4f}, Bal.Acc={test_result_dict['mean_balanced_acc']:.4f}\")\n        all_model_fold_results[model_name].append(test_result_dict)\n\n# --- 聚合和报告最终结果 ---\nprint(\"\\n===== 5折交叉验证完成，正在聚合结果... =====\")\n\naggregated_results = {}\nfor model_name, fold_results in all_model_fold_results.items():\n    # 使用DataFrame方便地计算均值和标准差\n    df = pd.DataFrame(fold_results)\n    aggregated_results[model_name] = {\n        metric: {'mean': df[metric].mean(), 'std': df[metric].std()}\n        for metric in df.columns\n    }\n\n# --- 打印最终的性能总结表格 ---\nsummary_list = []\nfor model_name, metrics in aggregated_results.items():\n    entry = {'Model': model_name}\n    for metric_name, values in metrics.items():\n        entry[f\"{metric_name} (Mean)\"] = values['mean']\n        entry[f\"{metric_name} (Std)\"] = values['std']\n    summary_list.append(entry)\n\nperformance_summary_df = pd.DataFrame(summary_list)\n# 格式化输出，使其更易读\nformatted_df = performance_summary_df[['Model']].copy()\nfor metric in aggregated_results[list(model_configs.keys())[0]].keys():\n    mean_col = f\"{metric} (Mean)\"\n    std_col = f\"{metric} (Std)\"\n    formatted_df[metric] = performance_summary_df.apply(\n        lambda row: f\"{row[mean_col]:.4f} ± {row[std_col]:.4f}\", axis=1\n    )\n\nprint(\"\\n--- 最终性能总结 (Mean ± Std over 5 Folds) ---\")\nprint(formatted_df.to_string())\n\n# 保存详细和格式化的结果到CSV\nperformance_summary_df.to_csv('results/overall_performance_summary_detailed.csv', index=False)\nformatted_df.to_csv('results/overall_performance_summary_formatted.csv', index=False)\nprint(\"\\n详细和格式化的性能总结已保存到 'results/' 文件夹中。\")\n\n# 绘制最终的性能对比图\nif aggregated_results:\n    plot_performance_bar_chartjs(aggregated_results)\n    print(\"最终性能对比图 (Chart.js config) 已保存到 results/overall_performance_summary_chartjs.json\")\n\nprint(\"\\n--- 脚本运行完毕 ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T01:58:26.948461Z","iopub.execute_input":"2025-08-27T01:58:26.948827Z","iopub.status.idle":"2025-08-27T01:59:30.656617Z","shell.execute_reply.started":"2025-08-27T01:58:26.948801Z","shell.execute_reply":"2025-08-27T01:59:30.655701Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2714828326.py:120: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  X_processed[col].fillna(X_processed[col].median(), inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"\n===== 开始处理第 1/5 折 =====\n--- 正在处理模型: LogisticRegression (第 1 折) ---\n    LogisticRegression (第 1 折) 测试结果: ROC-AUC=0.6348, Bal.Acc=0.5265\n--- 正在处理模型: XGBoost (第 1 折) ---\n    XGBoost (第 1 折) 测试结果: ROC-AUC=0.6699, Bal.Acc=0.5371\n--- 正在处理模型: CatBoost (第 1 折) ---\n    CatBoost (第 1 折) 测试结果: ROC-AUC=0.6869, Bal.Acc=0.5255\n--- 正在处理模型: MLP (第 1 折) ---\n    Epoch 50/150, Loss: 0.1027\n    Epoch 100/150, Loss: 0.0704\n    Epoch 150/150, Loss: -0.0627\n    MLP (第 1 折) 测试结果: ROC-AUC=0.6389, Bal.Acc=0.5906\n\n===== 开始处理第 2/5 折 =====\n--- 正在处理模型: LogisticRegression (第 2 折) ---\n    LogisticRegression (第 2 折) 测试结果: ROC-AUC=0.6781, Bal.Acc=0.5397\n--- 正在处理模型: XGBoost (第 2 折) ---\n    XGBoost (第 2 折) 测试结果: ROC-AUC=0.6322, Bal.Acc=0.5362\n--- 正在处理模型: CatBoost (第 2 折) ---\n    CatBoost (第 2 折) 测试结果: ROC-AUC=0.6624, Bal.Acc=0.5376\n--- 正在处理模型: MLP (第 2 折) ---\n    Epoch 50/150, Loss: 0.1049\n    Epoch 100/150, Loss: 0.0338\n    Epoch 150/150, Loss: -0.1588\n    MLP (第 2 折) 测试结果: ROC-AUC=0.6766, Bal.Acc=0.5566\n\n===== 开始处理第 3/5 折 =====\n--- 正在处理模型: LogisticRegression (第 3 折) ---\n    LogisticRegression (第 3 折) 测试结果: ROC-AUC=0.6623, Bal.Acc=0.5460\n--- 正在处理模型: XGBoost (第 3 折) ---\n    XGBoost (第 3 折) 测试结果: ROC-AUC=0.6620, Bal.Acc=0.5399\n--- 正在处理模型: CatBoost (第 3 折) ---\n    CatBoost (第 3 折) 测试结果: ROC-AUC=0.6917, Bal.Acc=0.5335\n--- 正在处理模型: MLP (第 3 折) ---\n    Epoch 50/150, Loss: 0.0847\n    Epoch 100/150, Loss: -0.0338\n    Epoch 150/150, Loss: -0.2493\n    MLP (第 3 折) 测试结果: ROC-AUC=0.6826, Bal.Acc=0.5420\n\n===== 开始处理第 4/5 折 =====\n--- 正在处理模型: LogisticRegression (第 4 折) ---\n    LogisticRegression (第 4 折) 测试结果: ROC-AUC=0.6394, Bal.Acc=0.5375\n--- 正在处理模型: XGBoost (第 4 折) ---\n    XGBoost (第 4 折) 测试结果: ROC-AUC=0.6423, Bal.Acc=0.5282\n--- 正在处理模型: CatBoost (第 4 折) ---\n    CatBoost (第 4 折) 测试结果: ROC-AUC=0.6686, Bal.Acc=0.5241\n--- 正在处理模型: MLP (第 4 折) ---\n    Epoch 50/150, Loss: 0.0939\n    Epoch 100/150, Loss: -0.0092\n    Epoch 150/150, Loss: -0.2088\n    MLP (第 4 折) 测试结果: ROC-AUC=0.6310, Bal.Acc=0.5519\n\n===== 开始处理第 5/5 折 =====\n--- 正在处理模型: LogisticRegression (第 5 折) ---\n    LogisticRegression (第 5 折) 测试结果: ROC-AUC=0.6973, Bal.Acc=0.5464\n--- 正在处理模型: XGBoost (第 5 折) ---\n    XGBoost (第 5 折) 测试结果: ROC-AUC=0.6965, Bal.Acc=0.5421\n--- 正在处理模型: CatBoost (第 5 折) ---\n    CatBoost (第 5 折) 测试结果: ROC-AUC=0.6992, Bal.Acc=0.5293\n--- 正在处理模型: MLP (第 5 折) ---\n    Epoch 50/150, Loss: 0.0888\n    Epoch 100/150, Loss: -0.0337\n    Epoch 150/150, Loss: -0.2442\n    MLP (第 5 折) 测试结果: ROC-AUC=0.6937, Bal.Acc=0.5745\n\n===== 5折交叉验证完成，正在聚合结果... =====\n\n--- 最终性能总结 (Mean ± Std over 5 Folds) ---\n                Model         mean_acc      mean_recall mean_specificity          mean_ap     mean_roc_auc mean_balanced_acc\n0  LogisticRegression  0.9126 ± 0.0024  0.0969 ± 0.0157  0.9816 ± 0.0008  0.1629 ± 0.0122  0.6624 ± 0.0263   0.5392 ± 0.0081\n1             XGBoost  0.9206 ± 0.0029  0.0854 ± 0.0103  0.9880 ± 0.0014  0.1658 ± 0.0113  0.6606 ± 0.0251   0.5367 ± 0.0053\n2            CatBoost  0.9253 ± 0.0018  0.0634 ± 0.0111  0.9965 ± 0.0010  0.1929 ± 0.0210  0.6818 ± 0.0156   0.5300 ± 0.0056\n3                 MLP  0.8801 ± 0.0059  0.1829 ± 0.0457  0.9433 ± 0.0081  0.1611 ± 0.0182  0.6646 ± 0.0278   0.5631 ± 0.0194\n\n详细和格式化的性能总结已保存到 'results/' 文件夹中。\n最终性能对比图 (Chart.js config) 已保存到 results/overall_performance_summary_chartjs.json\n\n--- 脚本运行完毕 ---\n","output_type":"stream"}],"execution_count":5}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "!pip install torch-geometric ucimlrepo imbalanced-learn\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, precision_recall_curve,average_precision_score, balanced_accuracy_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.utils import to_undirected, coalesce, remove_self_loops\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import json\n",
    "from scipy.stats import linregress\n",
    "import copy\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, balanced_accuracy_score, recall_score, f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TemporalFusion3(nn.Module):\n",
    "    \"\"\"\n",
    "    [Model Code Withheld for Peer Review]\n",
    "\n",
    "    The implementation of the TemporalFusion3 model is currently withheld\n",
    "    for the double-blind peer review process. The full source code will be made \n",
    "    publicly available upon the acceptance of the associated research paper.\n",
    "\n",
    "    This placeholder class is provided to ensure that the surrounding training \n",
    "    and evaluation framework remains executable. It accepts the same parameters\n",
    "    as the original model but contains a simplified forward pass that does not \n",
    "    reflect the model's true architecture or performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_total_features, num_classes, **kwargs):\n",
    "        # All original model parameters are accepted via **kwargs to maintain \n",
    "        # compatibility with the training script, but they are not used here.\n",
    "        super().__init__()\n",
    "        \n",
    "        # A simple linear layer is used as a placeholder for the complex\n",
    "        # architecture of the original model. This ensures the output shape\n",
    "        # is correct for the loss function.\n",
    "        self.placeholder_fc = nn.Linear(num_total_features, num_classes)\n",
    "        print(\"NOTE: Using a placeholder for TemporalFusion3. Model code is withheld for peer review.\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        # The original model performed dynamic graph construction, temporal feature\n",
    "        # fusion, and graph transformer layers.\n",
    "        # This placeholder performs a minimal operation to allow the script to run.\n",
    "        x = data.x\n",
    "        \n",
    "        # The original forward pass included dynamic graph generation like this:\n",
    "        # x_for_graph = x[:, self.indices_for_graph_construction]\n",
    "        # edge_index = self.generate_optimized_knn_graph(x_for_graph)\n",
    "        # This placeholder omits graph-based operations.\n",
    "\n",
    "        # The forward pass returns a tensor of the correct shape [batch_size, num_classes]\n",
    "        return self.placeholder_fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define device ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Loss Function ---\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=3.0, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        current_pos_weight = self.pos_weight\n",
    "        if current_pos_weight is not None and current_pos_weight.device != inputs.device:\n",
    "            current_pos_weight = current_pos_weight.to(inputs.device)\n",
    "\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none', pos_weight=current_pos_weight)\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
    "        focal_weight = alpha_t * torch.pow(1 - pt, self.gamma)\n",
    "        loss = focal_weight * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "myocardial_infarction_complications = fetch_ucirepo(id=579)\n",
    "X_df_orig = myocardial_infarction_complications.data.features.copy()\n",
    "y_df_orig = myocardial_infarction_complications.data.targets.copy()\n",
    "target_names = list(y_df_orig.columns)\n",
    "\n",
    "feature_collection_time = {\n",
    "    'AGE': 'admission', 'SEX': 'admission', 'INF_ANAM': 'admission', 'STENOK_AN': 'admission',\n",
    "    'FK_STENOK': 'admission', 'IBS_POST': 'admission', 'IBS_NASL': 'admission', 'GB': 'admission',\n",
    "    'SIM_GIPERT': 'admission', 'DLIT_AG': 'admission', 'ZSN_A': 'admission', 'nr_11': 'admission',\n",
    "    'nr_01': 'admission', 'nr_02': 'admission', 'nr_03': 'admission', 'nr_04': 'admission',\n",
    "    'nr_07': 'admission', 'nr_08': 'admission', 'np_01': 'admission', 'np_04': 'admission',\n",
    "    'np_05': 'admission', 'np_07': 'admission', 'np_08': 'admission', 'np_09': 'admission',\n",
    "    'np_10': 'admission', 'endocr_01': 'admission', 'endocr_02': 'admission', 'endocr_03': 'admission',\n",
    "    'zab_leg_01': 'admission', 'zab_leg_02': 'admission', 'zab_leg_03': 'admission',\n",
    "    'zab_leg_04': 'admission', 'zab_leg_06': 'admission', 'S_AD_KBRIG': 'admission',\n",
    "    'D_AD_KBRIG': 'admission', 'S_AD_ORIT': 'admission', 'D_AD_ORIT': 'admission',\n",
    "    'O_L_POST': 'admission', 'K_SH_POST': 'admission', 'MP_TP_POST': 'admission',\n",
    "    'SVT_POST': 'admission', 'GT_POST': 'admission', 'FIB_G_POST': 'admission',\n",
    "    'ant_im': 'admission', 'lat_im': 'admission', 'inf_im': 'admission', 'post_im': 'admission',\n",
    "    'IM_PG_P': 'admission', 'ritm_ecg_p_01': 'admission', 'ritm_ecg_p_02': 'admission',\n",
    "    'ritm_ecg_p_04': 'admission', 'ritm_ecg_p_06': 'admission', 'ritm_ecg_p_07': 'admission',\n",
    "    'ritm_ecg_p_08': 'admission', 'n_r_ecg_p_01': 'admission', 'n_r_ecg_p_02': 'admission',\n",
    "    'n_r_ecg_p_03': 'admission', 'n_r_ecg_p_04': 'admission', 'n_r_ecg_p_05': 'admission',\n",
    "    'n_r_ecg_p_06': 'admission', 'n_r_ecg_p_08': 'admission', 'n_r_ecg_p_09': 'admission',\n",
    "    'n_r_ecg_p_10': 'admission', 'n_p_ecg_p_01': 'admission', 'n_p_ecg_p_03': 'admission',\n",
    "    'n_p_ecg_p_04': 'admission', 'n_p_ecg_p_05': 'admission', 'n_p_ecg_p_06': 'admission',\n",
    "    'n_p_ecg_p_07': 'admission', 'n_p_ecg_p_08': 'admission', 'n_p_ecg_p_09': 'admission',\n",
    "    'n_p_ecg_p_10': 'admission', 'n_p_ecg_p_11': 'admission', 'n_p_ecg_p_12': 'admission',\n",
    "    'fibr_ter_01': 'admission', 'fibr_ter_02': 'admission', 'fibr_ter_03': 'admission',\n",
    "    'fibr_ter_05': 'admission', 'fibr_ter_06': 'admission', 'fibr_ter_07': 'admission',\n",
    "    'fibr_ter_08': 'admission', 'GIPO_K': 'admission', 'K_BLOOD': 'admission',\n",
    "    'GIPER_NA': 'admission', 'NA_BLOOD': 'admission', 'ALT_BLOOD': 'admission',\n",
    "    'AST_BLOOD': 'admission', 'KFK_BLOOD': 'admission', 'L_BLOOD': 'admission',\n",
    "    'ROE': 'admission', 'TIME_B_S': 'admission', 'NA_KB': 'admission', 'NOT_NA_KB': 'admission',\n",
    "    'LID_KB': 'admission', 'NITR_S': 'admission', 'LID_S_n': 'admission',\n",
    "    'B_BLOK_S_n': 'admission', 'ANT_CA_S_n': 'admission', 'GEPAR_S_n': 'admission',\n",
    "    'ASP_S_n': 'admission', 'TIKL_S_n': 'admission', 'TRENT_S_n': 'admission',\n",
    "    'R_AB_1_n': 'day1', 'R_AB_2_n': 'day2', 'R_AB_3_n': 'day3',\n",
    "    'NA_R_1_n': 'day1', 'NA_R_2_n': 'day2', 'NA_R_3_n': 'day3',\n",
    "    'NOT_NA_1_n': 'day1', 'NOT_NA_2_n': 'day2', 'NOT_NA_3_n': 'day3',\n",
    "}\n",
    "\n",
    "admission_features_names = [f for f, t in feature_collection_time.items() if t == 'admission' and f in X_df_orig.columns]\n",
    "day1_features_names = [f for f, t in feature_collection_time.items() if t == 'day1' and f in X_df_orig.columns]\n",
    "day2_features_names = [f for f, t in feature_collection_time.items() if t == 'day2' and f in X_df_orig.columns]\n",
    "day3_features_names = [f for f, t in feature_collection_time.items() if t == 'day3' and f in X_df_orig.columns]\n",
    "\n",
    "if not (len(day1_features_names) == len(day2_features_names) == len(day3_features_names)):\n",
    "    print(\"Warning: Temporal features per day are not consistent. Ensure this is intended.\")\n",
    "\n",
    "# Generate derived temporal features\n",
    "temporal_base_features = ['R_AB', 'NA_R', 'NOT_NA']\n",
    "derived_features = []\n",
    "for base in temporal_base_features:\n",
    "    day1_col, day2_col, day3_col = f'{base}_1_n', f'{base}_2_n', f'{base}_3_n'\n",
    "    X_df_orig[f'{base}_diff_2_1'] = X_df_orig[day2_col] - X_df_orig[day1_col]\n",
    "    X_df_orig[f'{base}_diff_3_2'] = X_df_orig[day3_col] - X_df_orig[day2_col]\n",
    "    derived_features.extend([f'{base}_diff_2_1', f'{base}_diff_3_2'])\n",
    "    X_df_orig[f'{base}_mean'] = X_df_orig[[day1_col, day2_col, day3_col]].mean(axis=1)\n",
    "    derived_features.append(f'{base}_mean')\n",
    "    def compute_trend(row):\n",
    "        x, y = np.array([1, 2, 3]), row[[day1_col, day2_col, day3_col]].values\n",
    "        return linregress(x, y).slope if not np.any(np.isnan(y)) else 0.0\n",
    "    X_df_orig[f'{base}_trend'] = X_df_orig.apply(compute_trend, axis=1)\n",
    "    derived_features.append(f'{base}_trend')\n",
    "\n",
    "ordered_features_all = admission_features_names + day1_features_names + day2_features_names + day3_features_names + derived_features\n",
    "X_df_orig = X_df_orig[ordered_features_all]\n",
    "\n",
    "num_admission_features_model = len(admission_features_names) + len(derived_features)\n",
    "num_temporal_features_per_step_model = len(day1_features_names)\n",
    "num_time_steps_model = 3\n",
    "\n",
    "for col in ordered_features_all:\n",
    "    if X_df_orig[col].isnull().any():\n",
    "        X_df_orig[col].fillna(X_df_orig[col].median(), inplace=True)\n",
    "\n",
    "print(\"Processing target labels:\")\n",
    "let_is_idx = target_names.index('LET_IS') if 'LET_IS' in target_names else 11\n",
    "num_binary_classes = len(target_names)\n",
    "y_binary = np.zeros((y_df_orig.shape[0], num_binary_classes), dtype=int)\n",
    "\n",
    "for col_idx, col_name in enumerate(y_df_orig.columns):\n",
    "    y_df_orig[col_name].fillna(y_df_orig[col_name].median(), inplace=True)\n",
    "    y_df_orig[col_name] = y_df_orig[col_name].astype(int)\n",
    "    y_binary[:, col_idx] = (y_df_orig[col_name] > 0).astype(int)\n",
    "    if not np.array_equal(np.unique(y_df_orig[col_name]), np.unique(y_binary[:, col_idx])):\n",
    "        print(f\"  Target '{col_name}': Original unique values {np.unique(y_df_orig[col_name])} -> Binarized unique values {np.unique(y_binary[:, col_idx])}\")\n",
    "\n",
    "X_np_all, y_np_all = X_df_orig.values, y_binary\n",
    "\n",
    "graph_construction_feature_names = ['AGE', 'SEX', 'K_BLOOD']\n",
    "indices_for_graph_construction_model = [ordered_features_all.index(name) for name in graph_construction_feature_names if name in ordered_features_all]\n",
    "if not indices_for_graph_construction_model:\n",
    "    print(\"Warning: No graph construction features found by name. Using first 3 features as fallback.\")\n",
    "    indices_for_graph_construction_model = list(range(min(3, X_np_all.shape[1])))\n",
    "print(f\"Indices used for graph construction (in data.x): {indices_for_graph_construction_model}\")\n",
    "\n",
    "def create_patient_nodes(X_np_fold_scaled, y_np_fold_labels):\n",
    "    return [Data(x=torch.tensor(X_np_fold_scaled[i, :], dtype=torch.float).unsqueeze(0), \n",
    "                 y=torch.tensor(y_np_fold_labels[i, :], dtype=torch.float).unsqueeze(0)) \n",
    "            for i in range(X_np_fold_scaled.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_configs = {\n",
    "    'TemporalFusion3_v1': {\n",
    "        'model': TemporalFusion3,\n",
    "        'params': {\n",
    "            'num_encoder_layers': 3,\n",
    "            'hidden_channels_per_head': 16,\n",
    "            'heads_per_layer': 4,\n",
    "            'dropout_rate': 0.3,\n",
    "            'short_term_cnn_out_channels': 64,\n",
    "            'short_term_cnn_layers': 4,\n",
    "            'long_term_gru_hidden_dim': 16,\n",
    "            'long_term_gru_input_dim': 12,\n",
    "            'long_term_gru_num_layers': 1,\n",
    "            'temporal_embedding_dim': 32,\n",
    "            'k_min': 5,\n",
    "            'k_max': 15,\n",
    "            'sim_threshold': 0.5,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# --- Training Parameters ---\n",
    "k_folds = 5\n",
    "training_epochs = 300\n",
    "early_stopping_patience = 50\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 2e-3\n",
    "batch_size = 64\n",
    "\n",
    "# --- Define Fold Splits (fixed for reproducibility) ---\n",
    "stratify_on_labels = (y_np_all.sum(axis=1) > 0).astype(int)\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "global_fold_indices = list(skf.split(X_np_all, stratify_on_labels))\n",
    "\n",
    "# --- Main Training ---\n",
    "for backbone_name, config in backbone_configs.items():\n",
    "    print(f\"\\n--- Starting {k_folds}-Fold CV for Backbone: {backbone_name} ---\")\n",
    "    model_class = config['model']\n",
    "    model_params = config['params']\n",
    "    \n",
    "    MODEL_NAME_FOR_PATHS = f\"TemporalGraphTransformer_Binary_{backbone_name}\"\n",
    "    output_dir = f'/kaggle/working/{MODEL_NAME_FOR_PATHS}'\n",
    "    checkpoints_dir = os.path.join(output_dir, 'checkpoints')\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    all_folds_metrics = []\n",
    "    all_folds_train_logs = []\n",
    "    all_folds_val_logs = []\n",
    "    max_epochs_across_folds = 0\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(global_fold_indices):\n",
    "        print(f\"\\n--- Backbone: {backbone_name}, Fold {fold_idx + 1}/{k_folds} ---\")\n",
    "        X_train, X_val = X_np_all[train_idx], X_np_all[val_idx]\n",
    "        y_train, y_val = y_np_all[train_idx], y_np_all[val_idx]\n",
    "\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train_scaled = np.nan_to_num(scaler.transform(X_train), nan=0.0)\n",
    "        X_val_scaled = np.nan_to_num(scaler.transform(X_val), nan=0.0)\n",
    "\n",
    "        train_nodes = create_patient_nodes(X_train_scaled, y_train)\n",
    "        val_nodes = create_patient_nodes(X_val_scaled, y_val)\n",
    "\n",
    "        train_loader = DataLoader(train_nodes, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(val_nodes, batch_size=len(val_nodes) if val_nodes else 1, shuffle=False)\n",
    "\n",
    "        model = model_class(\n",
    "            num_total_features=X_train_scaled.shape[1],\n",
    "            num_admission_features=num_admission_features_model,\n",
    "            num_temporal_features_per_step=num_temporal_features_per_step_model,\n",
    "            num_time_steps=num_time_steps_model,\n",
    "            indices_for_graph_construction=indices_for_graph_construction_model,\n",
    "            num_classes=num_binary_classes,\n",
    "            **model_params\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        pos_weight = torch.tensor( (len(y_train) - y_train.sum(axis=0)) / (y_train.sum(axis=0) + 1e-6), dtype=torch.float).to(device)\n",
    "        criterion = FocalLoss(alpha=0.5, gamma=2.0, pos_weight=pos_weight)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=15)\n",
    "\n",
    "        def train_epoch(loader, model, crit, opt):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for data in loader:\n",
    "                data = data.to(device)\n",
    "                opt.zero_grad()\n",
    "                out = model(data)\n",
    "                loss = crit(out, data.y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                opt.step()\n",
    "                total_loss += loss.item() * data.num_graphs\n",
    "            return total_loss / len(loader.dataset)\n",
    "\n",
    "        def test_epoch(loader, model, crit):\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for data in loader:\n",
    "                    data = data.to(device)\n",
    "                    out = model(data)\n",
    "                    preds = torch.sigmoid(out)\n",
    "                    all_preds.append(preds.cpu().numpy())\n",
    "                    all_labels.append(data.y.cpu().numpy())\n",
    "            return np.concatenate(all_preds), np.concatenate(all_labels)\n",
    "\n",
    "        best_val_auc = -1.0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        fold_train_log, fold_val_log = [], []\n",
    "        \n",
    "        for epoch in range(1, training_epochs + 1):\n",
    "            train_loss = train_epoch(train_loader, model, criterion, optimizer)\n",
    "            val_preds_prob, val_labels = test_epoch(val_loader, model, criterion)\n",
    "            val_preds_binary = (val_preds_prob > 0.5).astype(int)\n",
    "            \n",
    "            val_metrics = calculate_fold_metrics(val_labels, val_preds_prob, val_preds_binary, num_binary_classes, target_names, let_is_idx)\n",
    "            current_val_auc = val_metrics.get('mean_roc_auc', -1.0)\n",
    "            \n",
    "            fold_train_log.append({'avg_loss': train_loss})\n",
    "            fold_val_log.append(val_metrics)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Fold {fold_idx+1}, Epoch {epoch:03d}: Train Loss: {train_loss:.4f}, Val AUC: {current_val_auc:.4f}, Val Bal. Acc: {val_metrics.get(\"mean_balanced_acc\", -1):.4f}')\n",
    "            \n",
    "            scheduler.step(current_val_auc)\n",
    "            \n",
    "            if current_val_auc > best_val_auc:\n",
    "                best_val_auc = current_val_auc\n",
    "                patience_counter = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                all_folds_metrics.append(val_metrics)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch}. Best Val AUC: {best_val_auc:.4f}')\n",
    "                break\n",
    "        \n",
    "        max_epochs_across_folds = max(max_epochs_across_folds, epoch)\n",
    "        all_folds_train_logs.append(fold_train_log)\n",
    "        all_folds_val_logs.append(fold_val_log)\n",
    "        \n",
    "        if best_model_state:\n",
    "            checkpoint_path = os.path.join(checkpoints_dir, f'fold_{fold_idx}_best_model.pth')\n",
    "            torch.save(best_model_state, checkpoint_path)\n",
    "            print(f\"Fold {fold_idx+1} best model saved. AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "    # --- Post-Training Summary ---\n",
    "    print(f\"\\n--- Averaged K-Fold Performance for {backbone_name} ---\")\n",
    "    avg_metrics = {key: np.nanmean([m.get(key, np.nan) for m in all_folds_metrics]) for key in all_folds_metrics[0] if isinstance(all_folds_metrics[0][key], (int, float))}\n",
    "    for key, value in avg_metrics.items():\n",
    "        print(f\"  Avg {key.replace('_', ' ').title()}: {value:.4f}\")\n",
    "    \n",
    "    # --- Save Best Models to Final Directory ---\n",
    "    BEST_MODELS_FINAL_DIR = '/kaggle/working/best_models'\n",
    "    os.makedirs(BEST_MODELS_FINAL_DIR, exist_ok=True)\n",
    "    for i in range(k_folds):\n",
    "        src_path = os.path.join(checkpoints_dir, f'fold_{i}_best_model.pth')\n",
    "        dest_path = os.path.join(BEST_MODELS_FINAL_DIR, f'fold_{i}.pth')\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dest_path)\n",
    "    print(f\"\\nBest models from each fold copied to {BEST_MODELS_FINAL_DIR}\")\n",
    "    \n",
    "    # --- Plotting Training Curves ---\n",
    "    if all_folds_train_logs:\n",
    "        plot_kfold_training_curves(MODEL_NAME_FOR_PATHS, all_folds_train_logs, all_folds_val_logs, k_folds, max_epochs_across_folds)\n",
    "        print(f\"Training curves saved in /kaggle/working/{MODEL_NAME_FOR_PATHS}/\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
